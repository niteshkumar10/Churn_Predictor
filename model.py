# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U6tP6vTacan66E2Kj0t1HF3tlnro0Uqy
"""

# Importing necessary files
import os
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn import preprocessing
from scipy.stats import norm
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, roc_auc_score, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from thundersvm import SVC
from catboost import CatBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score,recall_score
from xgboost import XGBClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score, GridSearchCV
import pickle

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# Creating Path variables
path_models = '/content/drive/MyDrive/Colab_Notebooks/model'
path_dataset= '/content/drive/MyDrive/Colab_Notebooks/customer_churn_large_dataset.xlsx'
path_final_models = '/content/drive/MyDrive/Colab_Notebooks/final_model'

# loading the data set
table=pd.read_excel(path_dataset)

"""## **Initial Data Exploration**"""

# finding shape of the table that is number of rows and columns
table.shape

# Checking for null values
table.info()

# Checking for duplicate row
any(table.duplicated())

# determining number of unique values in each column
table.apply(pd.Series.nunique)

# from this we can conclude that the Customer_Id and Name are unique for every person and we can drop them
table=table.drop(['CustomerID','Name'],axis=1)

# description of each column of the table
table.describe()

# calculating mean median mode of all the columns
table.mean()

table.median()

table.mode()

table.Location.unique()

#feature mapping: Mapping the values to numerical equivalents for easy calculations
mapping_dict={"Gender":{"Male":0,"Female":1},"Location":{"Chicago":0,"Houston":1,"Los Angeles":2,"Miami":3,"New York":4}}
table=table.replace(mapping_dict)

# Heat map to determine correlation between various features of the data set
k = 7
cols = table.corr().nlargest(k, 'Churn')['Churn'].index
cm = table[cols].corr()
plt.figure(figsize=(10,6))
sns.heatmap(cm, annot=True, cmap = 'viridis')

"""From the above heat map plot we can conclude that correlation between all the features of the data set is under the considerable limit."""

# Determining distribution of all the features of the table
for i in table.columns:
    mu, std = norm.fit(table[i])
    plt.hist(table[i],density=True, alpha=0.6,color='lightblue', edgecolor = 'red')
    xmin, xmax = plt.xlim()
    x = np.linspace(xmin, xmax, 100)
    p = norm.pdf(x, mu, std)
    plt.plot(x, p, 'k', linewidth=2)
    title = i.format(mu, std)
    plt.title(title)
    plt.show()

# Box and Whisker  plot
sns.set_style("darkgrid")
ax = sns.boxplot(data=table, orient="h", palette="Set3")

# column wise Box and Whisker plot
column_name = table.columns
for i in column_name:
 fig = plt.figure(figsize =(8, 4))
 plt.subplot()
 sns.set_style("darkgrid")
 ax = sns.boxplot(data=table[i],orient="h",color='lightblue')
 ax.set_title(i)
 plt.show()

"""From the above box and whisker plot it is quite evident that there are no outliers. But to be on the safe side we will check for outliers using mathematical formulas

### **Checking for Outliers**
"""

# To determine the threshold value for outliers
def outlier_thresholds(dataframe, variable, low_quantile=0.05, up_quantile=0.95):
    quantile_one = dataframe[variable].quantile(low_quantile)
    quantile_three = dataframe[variable].quantile(up_quantile)
    interquantile_range = quantile_three - quantile_one
    up_limit = quantile_three + 1.5 * interquantile_range
    low_limit = quantile_one - 1.5 * interquantile_range
    return low_limit, up_limit

# Are there any outliers in the variables
def has_outliers(dataframe, numeric_columns, plot=False):
    for col in numeric_columns:
        low_limit, up_limit = outlier_thresholds(dataframe, col)
        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):
            number_of_outliers = dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].shape[0]
            print(col, " : ", number_of_outliers, "outliers")
            if plot:
                sns.boxplot(x=dataframe[col])
                plt.show()

for var in table:
    print(var, "has " , has_outliers(table, [var]),  "Outliers")

# As we know the dataset doesnt have any outliers so we prefer to use min max normalisation
min_max_scaler = preprocessing.MinMaxScaler()
table[['Age', 'Gender', 'Location', 'Subscription_Length_Months',
       'Monthly_Bill', 'Total_Usage_GB', 'Churn']] = min_max_scaler.fit_transform(table[['Age', 'Gender', 'Location', 'Subscription_Length_Months',
       'Monthly_Bill', 'Total_Usage_GB', 'Churn']])

"""As we have only 7 features only so there in no need for dimensionality reduction"""

#we will check for class imbalance
table['Churn'].value_counts()

"""The number of both the classes are equal so there is no class imbalance

## Model Building
"""

if not os.path.exists(path_models):
    if os.name == 'posix':
      !mkdir -p {path_models}

"""Setting up to use CUDA"""

!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb

!ls  ## Check if required cuda 9.0 amd64-deb file is downloaded

!dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb

!ls /var/cuda-repo-9-0-local | grep .pub

!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub

!apt-get update

!sudo apt-get install cuda-9.0

!nvcc --version

!pip install thundersvm

X = table.drop("Churn",axis=1)
y = table["Churn"]
# Train-Test Separation
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)
# Models for Classification
models = [('LR', LogisticRegression(random_state=123456)),
          ('KNN', KNeighborsClassifier()),
          ('CART', DecisionTreeClassifier(random_state=123456)),
          ('RF', RandomForestClassifier(random_state=123456)),
          ('SVR', SVC(gamma='auto',probability=True, random_state=123456)),
          ('GB', GradientBoostingClassifier(random_state = 12345)),
          ("LightGBM", LGBMClassifier(random_state=123456))]
results = []
names = []
for name, initiator in models:
    model=initiator.fit(x_train,y_train)
    kfold = KFold(n_splits=10)
    cv_results = cross_val_score(initiator, X, y, cv=kfold)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)
    model_pkl_file = path_models +"/"+ name +".pkl"
    with open(model_pkl_file, 'wb') as file:
        pickle.dump(model, file)

# Auc Roc Curve
def generate_auc_roc_curve(clf, X_test):
    y_pred_proba = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)
    plt.plot(fpr,tpr,label="AUC ROC Curve with Area Under the curve ="+str(auc))
    plt.legend(loc=4)
    plt.show()
    pass

# Confusion Matrix
model_name=['LR','KNN','CART','RF','SVR','GB',"LightGBM"]
for name in model_name:
    model_pkl_file=path_models+"/"+name+".pkl"
    with open(model_pkl_file, 'rb') as file:
        model = pickle.load(file)
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    conf_mat = confusion_matrix(y_pred,y_test)
    print(name)
    conf_mat
    print("\n")
    print("True Positive : ", conf_mat[1, 1])
    print("True Negative : ", conf_mat[0, 0])
    print("False Positive: ", conf_mat[0, 1])
    print("False Negative: ", conf_mat[1, 0])
    print("\n  Classification Report")
    print(classification_report(model.predict(x_test),y_test))
    print("\n AUC Curve")
    generate_auc_roc_curve(model, x_test)

"""### **Model Tunning**

Based on the AOC ROC curve we will use Light BGM model for prediction
"""

if not os.path.exists(path_final_models):
    if os.name == 'posix':
      !mkdir -p {path_final_models}

# LightGBM:
lgb_model = LGBMClassifier()
# Model Tuning
lgbm_params = {'colsample_bytree': 0.5,
 'learning_rate': 0.01,
 'max_depth': 6,
 'n_estimators': 500}

lgbm_tuned = LGBMClassifier(**lgbm_params).fit(X, y)
name='LightGBM'

# saving the final mode
model_pkl_file = path_final_models +"/"+ name +".pkl"
with open(model_pkl_file, 'wb') as file:
  pickle.dump(model, file)


kfold = KFold(n_splits=10)
cv_results = cross_val_score(model, X, y, cv=10, scoring="accuracy")
msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
print(msg)

# checking the final model performanc
model_pkl_file=path_final_models+"/"+name+".pkl"
with open(model_pkl_file, 'rb') as file:
  model = pickle.load(file)
base = model.fit(x_train,y_train)
y_pred = base.predict(x_test)
acc_score = accuracy_score(y_test, y_pred)
feature_imp = pd.Series(base.feature_importances_,
                        index=X.columns).sort_values(ascending=False)

sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Score')
plt.ylabel('Variable')
plt.title(name)
plt.show()


